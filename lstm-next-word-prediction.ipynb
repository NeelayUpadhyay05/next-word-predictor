{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3c9a9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Sherlock_Holmes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = [line for line in f.read().splitlines() if line.strip()] # Storing sentences of the dataset in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "742a0d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # Importing the Tokenizer class from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5e54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() # Initializing the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1caf496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(corpus) # Fitting the tokenizer on the corpus to create a word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ca57bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating input sequences for the model\n",
    "input_sequences = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
    "\n",
    "    for i in range(1, len(tokenized_sentence)):\n",
    "        input_sequences.append(tokenized_sentence[:i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06477967",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(x) for x in input_sequences]) # Finding the maximum length of the input sequences to pad them to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed82bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded_input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding=\"pre\") # Padding the input sequences to the same length using pre-padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e7c0a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the padded input sequences into features (X) and labels (y)\n",
    "X = padded_input_sequences[:, :-1]\n",
    "y = padded_input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ebd502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "vocab_size = len(tokenizer.word_index) + 1 # Calculating the vocabulary size\n",
    "y = to_categorical(y, num_classes=vocab_size) # One-hot encoding the labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (projects)",
   "language": "python",
   "name": "projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
